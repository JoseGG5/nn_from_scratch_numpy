{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a6d264e-3b39-41b9-af3c-61837c6e99a3",
   "metadata": {},
   "source": [
    "# Contruye tu primera red neuronal desde cero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eab44e1-f789-4a7a-b9d9-c7bd9506a9a4",
   "metadata": {},
   "source": [
    "La IA ha revolucionado al mundo, haciendo posible avances en ciertos campos que antes se veían a años luz. Hay mucho ruido acerca de \"palabros\" raros como Transformers, Atención, Embeddings, CNNs..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3cfc98-eb78-473c-8f60-ab67b6fb93d5",
   "metadata": {},
   "source": [
    "Todo eso está genial, y es el estado del arte sobre el que se apoya todo lo que usamos a día de hoy (por ejemplo, los LLM o modelos de visión). Sin embargo, todo esto esta cimentado por una base muy sólida, y, he prepado este notebook para que todos salgais de esta charla conociendola."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e0a0e3-6468-4cfc-9b4a-b10727d50bd1",
   "metadata": {},
   "source": [
    "Mi objetivo es que al salir todos comprendais: \n",
    "1. ¿Qué es una red neuronal?\n",
    "2. ¿Cómo aprende una red neuronal?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27feaf47-23c1-426f-85bd-59bfbcf96f57",
   "metadata": {},
   "source": [
    "Me he dado cuenta en este tiempo que usar un modelo realmente no es tan complejo, lo complejo es comprenderlo e implementarlo por ti mismo. Es por esto que, inspirado por la frase de Richard Feynman \"Lo que no puedo crear, no lo entiendo\", os propongo que creeis con ayuda de este notebook y mia vuestra primera red neuronal desde cero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5138f45-48fe-432f-8ddb-e91c32701ca9",
   "metadata": {},
   "source": [
    "Cuando digo desde cero es desde cero, no vamos a usar ningun framework super conocido, como puede ser PyTorch o Tensorflow. Nos apoyaremos en Python puro y un paquete que sirve para trabajar con arrays llamado \"numpy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a90e1-3d83-4eec-b4e9-998f157d9bc1",
   "metadata": {},
   "source": [
    "Antes de empezar, vamos a instalar los paquetes necesarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91bcf7a3-8de1-40d3-ad2e-1afe54fe1726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\jose antonio\\desktop\\scriptsproyectos\\own\\youtube\\make_your_first_net\\.venv\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jose antonio\\desktop\\scriptsproyectos\\own\\youtube\\make_your_first_net\\.venv\\lib\\site-packages (3.9.4)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\jose antonio\\desktop\\scriptsproyectos\\own\\youtube\\make_your_first_net\\.venv\\lib\\site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jose antonio\\desktop\\scriptsproyectos\\own\\youtube\\make_your_first_net\\.venv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jose antonio\\desktop\\scriptsproyectos\\own\\youtube\\make_your_first_net\\.venv\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jose antonio\\desktop\\scriptsproyectos\\own\\youtube\\make_your_first_net\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jose antonio\\desktop\\scriptsproyectos\\own\\youtube\\make_your_first_net\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jose antonio\\desktop\\scriptsproyectos\\own\\youtube\\make_your_first_net\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jose antonio\\desktop\\scriptsproyectos\\own\\youtube\\make_your_first_net\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jose antonio\\desktop\\scriptsproyectos\\own\\youtube\\make_your_first_net\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jose antonio\\desktop\\scriptsproyectos\\own\\youtube\\make_your_first_net\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jose antonio\\desktop\\scriptsproyectos\\own\\youtube\\make_your_first_net\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\jose antonio\\desktop\\scriptsproyectos\\own\\youtube\\make_your_first_net\\.venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jose antonio\\desktop\\scriptsproyectos\\own\\youtube\\make_your_first_net\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Jose Antonio\\Desktop\\ScriptsProyectos\\own\\youtube\\make_your_first_net\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e42d2d6-426c-405b-82a0-6c10d94e6c7d",
   "metadata": {},
   "source": [
    "### ¿Qué es una red neuronal?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4274f1a5-49b4-4552-b256-8fa6e719f358",
   "metadata": {},
   "source": [
    "Una red neuronal puede definirse como: \n",
    "1. Un modelo computacional inspirado en el cerebro humano, formado por nodos (neuronas artificiales) que procesan información y aprenden a partir de datos.\n",
    "2. Un conjunto de algoritmos diseñados para reconocer patrones mediante el ajuste de pesos que conectan múltiples capas de neuronas.\n",
    "3. Una estructura matemática que transforma entradas en salidas mediante funciones no lineales y aprendizaje automático.\n",
    "4. Un sistema de aprendizaje que aproxima funciones complejas aprendiendo representaciones internas de los datos.\n",
    "\n",
    "Y la lista sigue y sigue..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cbc7df-6205-4ff3-97a3-5af76bfb47e5",
   "metadata": {},
   "source": [
    "Con lo que teneis que quedaros es que una red neuronal no es más que un modelo que aprende patrones en datos y resuelve tareas como clasificación o predicción. El ejemplo mas sencillo, y que nos sirve para aprender la base de todo es el perceptron multicapa, que fue propuesto en 1958 por Frank Rosenblatt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a799d8-070b-4ad6-be8a-bb499588d0c6",
   "metadata": {},
   "source": [
    "#### Perceptron multicapa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5c37d-7370-4ee8-85a3-81169acb98c8",
   "metadata": {},
   "source": [
    "![MLP](images\\mlp_basico_anotado.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a57684-e8dd-4f6a-b734-ed136dd2dc0e",
   "metadata": {},
   "source": [
    "A pesar del jaleo que se ve ahí, esto es realmente sencillo. Esta red esta formada por 3 tipos de capas como podemos ver, veamos que hace cada una:\n",
    "1. Capa de entrada: Esta capa no es más que la capa que recibe los datos y los pasa a la siguiente capa, así de sencillo.\n",
    "2. Capas ocultas: Recibe la salida de la capa anterior, calcula combinaciones ponderadas, e introducen no linealidad (de esta forma se aprenden relaciones complejas)\n",
    "3. Capa de salida: Reciben la salida de la última capa oculta y generan el resultado final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873a92e-f752-4730-998c-432f61342e7a",
   "metadata": {},
   "source": [
    "### Cómo se entrena una red neuronal. Concepto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e256197-f765-4372-93d9-c263047dd309",
   "metadata": {},
   "source": [
    "Para entrenar cualquier tipo de red neuronal se usa un algoritmo conocido como \"descenso del gradiente\". La idea de este es muy sencilla a la vez que poderosa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eca18e-8ad4-4601-a8cb-2f0a5a5cc0fd",
   "metadata": {},
   "source": [
    "El algoritmo cuenta con tan solo dos pasos:\n",
    "1. Forward: En este, la red neuronal recibe un dato de entrada, y lo propaga hasta el final (se aplican todas las operaciones), obteniendose un valor de salida.\n",
    "2. Backward: Se mira como de acertada es la salida de la red, y, en base a esto, se calcula el gradiente de cada peso respecto de esa pérdida. Esto lo que nos dice es como hay que actualizar los pesos de nuestra red para disminuir el error de la salida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65718bd1-3cca-47d2-90ea-879aa6685d1a",
   "metadata": {},
   "source": [
    "#### Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c00af4-abf5-4b6d-8baa-a2b7737fb57f",
   "metadata": {},
   "source": [
    "Para entender el paso forward, es necesario entender cómo funciona cada parte de una red neuronal, y que operaciones se realizan. Para esto solo hay que entender que hace cada nodo o neurona de la red. Para esto vamos a coger de ejemplo la primera neurona de nuestra red y vamos a entender que ocurre cuando enviamos datos por las capas de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df05fdd-f7c1-4ce7-b261-e2463e6387b0",
   "metadata": {},
   "source": [
    "![Neuron](images\\neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793e4a3f-0786-4b7e-861b-d4297101a05d",
   "metadata": {},
   "source": [
    "La neurona $h1_1$ realiza lo que se llama una suma ponderada, es decir, coge cada una de las entradas que vienen de $x_1$, $x_2$ y $x_3$ y las multiplica por un peso $w_1$, $w_2$ y $w_3$, respectivamente. De esta forma: $$z = w_1x_1 + w_2x_2 + ... + w_nx_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef3210e-6f61-4340-9ddd-ada57b2548cd",
   "metadata": {},
   "source": [
    "Además de esto, generalmente las neuronas suman un término conocido como \"bias\". La función de este término es la de desplazar la frontera de decisión de nuestro modelo para que esta no pase siempre por el origen. En términos sencillos, si imaginamos que nuestra neurona es un interruptor que cuando la entrada pasa un umbral se activa, sería como darle la habilidad a este de ajustar el umbral. Por ende, la ecuación ahora queda: $$z = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa29ec16-0e8b-45ef-9782-d31f74a9363f",
   "metadata": {},
   "source": [
    "Pero... ¡esto no es más que una regresión lineal! ¿Dónde está lo novedoso?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c548909-2341-4f2e-beb7-45e129517257",
   "metadata": {},
   "source": [
    "![meme_lr](images\\linear_regression.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b10cd14-36cf-4124-a108-0f729616845b",
   "metadata": {},
   "source": [
    "La realidad es que hasta ahora no hay nada novedoso. Sin embargo, lo bueno que tienen las redes neuronales es que permiten aprender patrones no lineales en los datos. ¿Qué quiere decir esto? Y, ¿cómo van a encontrar patrones no lineales si hasta ahora son una simple regresión lineal?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e80b7-cad4-41d0-a49d-c87ec796ea0c",
   "metadata": {},
   "source": [
    "Veamos que quiere decir que encuentran patrones no lineales. Supongamos que tenemos este conjunto de datos, donde tenemos 2 clases, y tenemos que trazar una linea que separe los datos de una clase y de la otra. ¿Cómo lo haríais?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a2b9b-39bf-4429-8cdb-d9417f430491",
   "metadata": {},
   "source": [
    "![MLP](images\\decision_boundary_lineal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1a48ed-4a7d-49e1-ba24-09b1c29a198a",
   "metadata": {},
   "source": [
    "La realidad es que no se puede. No hay una linea que separe bien ambos conjuntos. Sin embargo, ¿y si os digo que teneis completa libertad para dibujar una frontera que los separe? ¿Y si ya no estais sujetos a usar una única línea? Esto, en resumidas cuentas, es el concepto de \"no linealidad\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d891e-0c18-4eb3-903e-184b7af0daa8",
   "metadata": {},
   "source": [
    "![MLP](images\\decision_boundary_nolineal.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1956167-e040-4382-810d-11d963381005",
   "metadata": {},
   "source": [
    "Como podemos ver, con no linealidad, somos de encontrar el patrón complejo que describen estos datos, y definir una frontera perfecta. De esta forma, si me viene un dato nuevo, se decir en que parte de la zona está, y, por ende, se clasificarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b1cea-1c58-4ad0-b427-afa64b3e072d",
   "metadata": {},
   "source": [
    "Ahora que sabemos que es la \"no linealidad\", vamos a entender como sacamos no linealidad de algo que hasta ahora es completamente lineal, como lo es la ecuación descrita antes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea8e4e-44b6-416d-b4d6-85b048cfd2bd",
   "metadata": {},
   "source": [
    "El truco es que todavía no hemos terminado de ver que hace una neurona. Una neurona, aplica tras hacer esa suma ponderada, una llamada \"función de activación\". Esta función de activación es una función no lineal, y, por eso, cuando la aplicamos, la red aprende patrones no lineales, porque la ecuación que describe lo que hace una neurona deja de ser lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324c4b3-9c79-4c47-a9e5-347b3e59a103",
   "metadata": {},
   "source": [
    "Hay muchas funciones de activación, pero la más tipica es la Rectified Linear Unit (ReLU). Esta viene descrita por la ecuación $$\n",
    "f(x) = \\max(0, x)\n",
    "$$ Que graficamente sería:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab74c2-b22f-431a-9a94-a637b9e496a8",
   "metadata": {},
   "source": [
    "![MLP](images\\relu.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb688da-dbfd-402d-9650-f3c178801c78",
   "metadata": {},
   "source": [
    "Como se puede ver, la función no es lineal, y por esto mismo, se introduce no linealidad en la red."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cad145-5900-4b6f-af24-988b69d5345e",
   "metadata": {},
   "source": [
    "Recapitulando entonces, la ecuación que describe la salida de nuestra neurona frente es: $$z = ReLU(w_1x_1 + w_2x_2 + ... + w_nx_n + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31b6793-c48b-4ad9-b92c-118eeb635032",
   "metadata": {},
   "source": [
    "Una vez comprendido esto, es sencillo crear el forward de nuestra red neuronal, pues simplemente hay que replicar este cálculo para todas las neuronas. ¡Procedemos a programar!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7e8f10-f46b-4cfb-a9d1-ba878ec632c0",
   "metadata": {},
   "source": [
    "## A construir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b8c1a-0e4e-40d1-9a4f-487fcb23523b",
   "metadata": {},
   "source": [
    "Cabe destacar que mi intención en esta charla no es la programación, osea que usaremos código sencillo y legible a pesar de que no sea robusto ni escalable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68edcae7-d09c-442a-8292-5f40251f19d9",
   "metadata": {},
   "source": [
    "La tarea que nuestra red usará es la de clasificar imagenes de dígitos escritos a mano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435a070-86d6-4f56-aa46-537c07e90ca6",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bcadbf-248d-4562-bafd-2eeb0bd9dc25",
   "metadata": {},
   "source": [
    "MNIST cuenta con imágenes de dígitos escritos a mano desde el 0 hasta el 9, por lo que tiene 10 clases entre las que clasificar. Las imagenes tienen un formato de (1x28x28), esto es, que son imagenes en escala de grises (un solo canal de color), 28 pixeles de ancho y 28 pixeles de alto. Empezamos por descargar el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e377cc2-20e7-49ae-a41e-4898de5b5c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (60000, 28, 28) (60000,)\n",
      "Test: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "path = \"mnist.npz\"\n",
    "\n",
    "urllib.request.urlretrieve(url, path)\n",
    "\n",
    "with np.load(path) as data:\n",
    "    x_train = data[\"x_train\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "    x_test  = data[\"x_test\"]\n",
    "    y_test  = data[\"y_test\"]\n",
    "\n",
    "print(\"Train:\", x_train.shape, y_train.shape)\n",
    "print(\"Test:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159f0d7-e9be-4e62-944c-aa3bb696f1a1",
   "metadata": {},
   "source": [
    "Como podemos ver, tenemos 60.000 imagenes en nuestro conjunto de datos para entrenamiento, y 10.000 imagenes para testear nuestro modelo. Vamos a observar como son estas imagenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f23ad2-243e-410f-a545-5354be422d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAb9klEQVR4nO3dC3BUVZ7H8X+HPAiPBEPIawgYQEBBcETA8DIYKhFqEZCxRJ0qsFwoGEAhPqjMKshoVUacVQsHodxyCFYhKrU8BkozA4GEQRIcQIZlGZFglFCQIKxJIEjI426dyybSGGBvk+Tf6fv9VN3qdPf9c08uN/3rc+/p0x7LsiwBAKCVBbX2BgEAMAggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCGgFeXl54vF4mlwKCwu1mweoCNbZLOBOzzzzjAwdOtTrsT59+qi1B9BEAAGtaPTo0fKrX/1KuxmAX+AUHNDKzp8/L7W1tdrNANQRQEAreuqppyQiIkLat28vY8eOlX379mk3CVDDKTigFYSGhsrUqVNlwoQJEh0dLUeOHJE//OEP9im5PXv2yC9/+UvtJgKtzsMX0gE6ioqKZNCgQTJmzBjJycnRbg7Q6jgFBygxo98mTZokO3fulLq6Ou3mAK2OAAIUJSYmyuXLl6Wqqkq7KUCrI4AARd988409IKFTp07aTQFaHQEEtILvv//+Z4/94x//kD//+c+SlpYmQUH8KcJ9GIQAtIIHH3xQwsPDZcSIERITE2OPgnvvvfckJCRECgoK5M4779RuItDqCCCgFSxfvlzWrl1rj3yrrKyUbt26SWpqqixZsoSpeOBaBBAAQAUnngEAKgggAIAKAggAoIIAAgCoIIAAACoIIACACr/7Oob6+no5deqUdO7cWTwej3ZzAAAOmU/3mC9eTEhIuOEsH34XQCZ8zASNAIC2raSkRLp37952Asj0fIxRMkGCJUS7OQAAh2qlRnbLp42v560eQCtWrJA33nhDSktLZfDgwfLOO+/IsGHDblrXcNrNhE+whwACgDbn/+bXudlllBYZhPDxxx9LRkaGPc/VgQMH7ABKT0+XM2fOtMTmAABtUIsE0JtvvikzZ86Up556Su666y5ZtWqVdOjQQf70pz+1xOYAAG1QsweQ+XbH/fv3y7hx437aSFCQfd9MO3+t6upqe3bgqxcAQOBr9gA6e/as/f32sbGxXo+b++Z60LWysrIkMjKycWEEHAC4g/oHUTMzM6WioqJxMcP2AACBr9lHwUVHR0u7du2krKzM63FzPy4u7mfrh4WF2QsAwF2avQcUGhoqQ4YMkdzcXK/ZDcz95OTk5t4cAKCNapHPAZkh2NOnT5f77rvP/uzP22+/LVVVVfaoOAAAWiyAHnvsMfn+++9l8eLF9sCDe+65R3Jycn42MAEA4F4ey8wa50fMMGwzGi5FJjETAgC0QbVWjeTJZntgWUREhP+OggMAuBMBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFcE6mwVaVumzI3yqs8b+4Lxm922Oa/599n84rknrUOO4ZlHZPeKL/QvvdVxT26Gd45qQ87WOa4L+9qXjGvgnekAAABUEEAAgMALolVdeEY/H47X079+/uTcDAGjjWuQa0IABA2T79u0/bSSYS00AAG8tkgwmcOLi4lrinwYABIgWuQZ07NgxSUhIkF69esmTTz4pJ06cuO661dXVUllZ6bUAAAJfswfQ8OHDJTs7W3JycmTlypVSXFwso0ePlvPnzze5flZWlkRGRjYuiYmJzd0kAIAbAmj8+PHy6KOPyqBBgyQ9PV0+/fRTKS8vl08++aTJ9TMzM6WioqJxKSkpae4mAQD8UIuPDujSpYv07dtXioqKmnw+LCzMXgAA7tLinwO6cOGCHD9+XOLj41t6UwAANwfQ888/L/n5+fLtt9/Knj17ZMqUKdKuXTt5/PHHm3tTAIA2rNlPwZ08edIOm3Pnzkm3bt1k1KhRUlhYaP8MAEADj2VZlvgRMwzbjIZLkUkS7AnRbg78QOVnvR3XbBiwxqdtRbcLF38VJB7HNesvdPVpW/1DSx3XPPP1NMc19Zbz36luTYzjmtIx9eKLvnO+8KnO7WqtGsmTzfbAsoiIiOuux1xwAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAAvML6YBbtXvQesc19eK/k4r66vVzdzqueT93rE/bGj/qS8c1nw5Y57jmf+prHdc8kLrAcU3w/zCxsT+iBwQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUMFs2PB739ZedFzTI7j1ZsN+oXS445rP/zjUcU30gXLHNf0rTjmuMY5uuMtxzZgB90pr6NzB47implOLNAW3iB4QAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFUxGCr837rMMxzVfT1wpreVQ5j2Oa6L+WuC4pt5xhW81RtC3JxzXdPubjxuDa9EDAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoILJSNGqyuaPcFxT/PC7jmvqLI+0lpNjQxzXJP21RZoCtCn0gAAAKgggAEDbCKBdu3bJxIkTJSEhQTwej2zatMnrecuyZPHixRIfHy/h4eEybtw4OXbsWHO2GQDgxgCqqqqSwYMHy4oVK5p8ftmyZbJ8+XJZtWqV7N27Vzp27Cjp6ely6dKl5mgvAMCtgxDGjx9vL00xvZ+3335bXnrpJZk0aZL92AcffCCxsbF2T2natGm33mIAQEBo1mtAxcXFUlpaap92axAZGSnDhw+XgoKmv4K4urpaKisrvRYAQOBr1gAy4WOYHs/VzP2G566VlZVlh1TDkpiY2JxNAgD4KfVRcJmZmVJRUdG4lJSUaDcJANDWAiguLs6+LSsr83rc3G947lphYWESERHhtQAAAl+zBlBSUpIdNLm5uY2PmWs6ZjRccnJyc24KAOC2UXAXLlyQoqIir4EHBw8elKioKOnRo4csWLBAXnvtNbnjjjvsQHr55ZftzwxNnjy5udsOAHBTAO3bt0/Gjh3beD8jI8O+nT59umRnZ8uLL75of1Zo1qxZUl5eLqNGjZKcnBxp375987YcANCmeSzz4R0/Yk7ZmdFwKTJJgj3OJ3lE6/GEhDqu+e7Dfo5rDo9Y47imXnw7rF8oHe645qv7nZ/JtmouO64B2opaq0byZLM9sOxG1/XVR8EBANyJAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIANA2vo4BaPDD40Mc1xwa8UcftuRxXDHx6MM+bEckaF5HxzVWzdc+bQtwO3pAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVDAZKcQT7NthkLrwc/FXJ3b09Kku8cieZm8LgKbRAwIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCyUghcnc/n8qWxnwg/qp7bpV2EwDcBD0gAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKpiMFBJUUupT3brzsY5rnux8xnFNO4/z90kh35aJL2p9qgLgC3pAAAAVBBAAoG0E0K5du2TixImSkJAgHo9HNm3a5PX8jBkz7MevXh566KHmbDMAwI0BVFVVJYMHD5YVK1Zcdx0TOKdPn25c1q1bd6vtBAC4fRDC+PHj7eVGwsLCJC4u7lbaBQAIcC1yDSgvL09iYmKkX79+MmfOHDl37tx1162urpbKykqvBQAQ+Jo9gMzptw8++EByc3Pl9ddfl/z8fLvHVFdX1+T6WVlZEhkZ2bgkJiY2d5MAAG74HNC0adMaf7777rtl0KBB0rt3b7tXlJqa+rP1MzMzJSMjo/G+6QERQgAQ+Fp8GHavXr0kOjpaioqKrnu9KCIiwmsBAAS+Fg+gkydP2teA4uPjW3pTAIBAPgV34cIFr95McXGxHDx4UKKiouxl6dKlMnXqVHsU3PHjx+XFF1+UPn36SHp6enO3HQDgpgDat2+fjB07tvF+w/Wb6dOny8qVK+XQoUOyZs0aKS8vtz+smpaWJq+++qp9qg0AAJ8DKCUlRSzLuu7zf/nLX5z+k1BWd/b6w+Rv5EDV7Y5rHu/sfJLQ3IshjmvOD+shvmh3qbv4q4pezvdD7J5yn7Z1Ka6j45rwL447rqn74QfHNQgczAUHAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUEEABABQEEAAiMr+SGe/x10zDHNW/M3uu4JjW82nHN2HdXSqAJEo/jmuO1P/q0raTg9o5rHv/G+Xd+HfvPEY5r4t7e47gG/okeEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFQQQAEAFAQQAUEEAAQBUEEAAABUey7Is8SOVlZUSGRkpKTJJgj0h2s3BDbTrEum45uE9RY5rZkaWOK6pF786rNUmIw3E/ZD8yjzHNV3f/8K3jdXX+VbncrVWjeTJZqmoqJCIiIjrrkcPCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAomI0WratcnyXFNlzXljmvW3L5dAs3OH9s7rnnuvx71aVuP9T7guGZR1/8Wf52U9eH7Jvi0rdrTpT7VuV0tk5ECAPwZAQQAUEEAAQBUEEAAABUEEABABQEEAFBBAAEAVBBAAAAVBBAAQAUBBABQQQABAFQQQAAAFcE6m4Vb1RUVO645N9L5dv5Fhvg2WWp0V8c1ZY/0dVwT/V6BtIYEOeJT3d/E+cSnu4fOcFwz6v2/O655KforxzXF/9pLfJH4KpORtiR6QAAAFQQQAMD/AygrK0uGDh0qnTt3lpiYGJk8ebIcPXrUa51Lly7J3LlzpWvXrtKpUyeZOnWqlJWVNXe7AQBuCqD8/Hw7XAoLC2Xbtm1SU1MjaWlpUlVV1bjOwoULZcuWLbJ+/Xp7/VOnTskjjzzSEm0HALhlEEJOTo7X/ezsbLsntH//fhkzZoz97Xfvv/++fPjhh/Lggw/a66xevVruvPNOO7Tuv//+5m09AMCd14BM4BhRUVH2rQki0ysaN25c4zr9+/eXHj16SEFB06N+qqur7a/hvnoBAAQ+nwOovr5eFixYICNHjpSBAwfaj5WWlkpoaKh06dLFa93Y2Fj7uetdV4qMjGxcEhMTfW0SAMANAWSuBR0+fFg++uijW2pAZmam3ZNqWEpKSm7p3wMABPAHUefNmydbt26VXbt2Sffu3Rsfj4uLk8uXL0t5eblXL8iMgjPPNSUsLMxeAADu4qgHZFmWHT4bN26UHTt2SFJSktfzQ4YMkZCQEMnNzW18zAzTPnHihCQnJzdfqwEA7uoBmdNuZoTb5s2b7c8CNVzXMdduwsPD7dunn35aMjIy7IEJERERMn/+fDt8GAEHAPA5gFauXGnfpqSkeD1uhlrPmHFlHqi33npLgoKC7A+gmhFu6enp8u677zrZDADABYKdnoK7mfbt28uKFSvsBWhr6s6e89uJRf1d2dDOjmvm3nbAcU2d5Xyi1Ni/1ziuQctjLjgAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAQNv5RlQAbUNw4k/fWOxE8Vs/faPx/1fusGWOazoFhTuuuW/fE45rYrd/Kb64+fz/uBX0gAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKhgMlJAwQ8zkh3XjJ6/13HNxC6fii9Gtq9xXHO2zvl2+m6Z47xm9heOa5hU1D/RAwIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCyUgBBRV3OK/ZVtLPcU1lbbjzDYnIWh9qvn5tgOOavlucTyyKwEEPCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAomIwUU3P5vBa2ynRPSetoLE4vCGXpAAAAVBBAAwP8DKCsrS4YOHSqdO3eWmJgYmTx5shw9etRrnZSUFPF4PF7L7Nmzm7vdAAA3BVB+fr7MnTtXCgsLZdu2bVJTUyNpaWlSVVXltd7MmTPl9OnTjcuyZcuau90AADcNQsjJyfG6n52dbfeE9u/fL2PGjGl8vEOHDhIXF9d8rQQABJxbugZUUVFh30ZFRXk9vnbtWomOjpaBAwdKZmamXLx48br/RnV1tVRWVnotAIDA5/Mw7Pr6elmwYIGMHDnSDpoGTzzxhPTs2VMSEhLk0KFDsmjRIvs60YYNG657XWnp0qW+NgMA0EZ5LMuyfCmcM2eOfPbZZ7J7927p3r37ddfbsWOHpKamSlFRkfTu3bvJHpBZGpgeUGJioqTIJAn2hPjSNACAolqrRvJks32WLCIionl7QPPmzZOtW7fKrl27bhg+xvDhw+3b6wVQWFiYvQAA3MVRAJnO0vz582Xjxo2Sl5cnSUlJN605ePCgfRsfH+97KwEA7g4gMwT7ww8/lM2bN9ufBSotLbUfj4yMlPDwcDl+/Lj9/IQJE6Rr1672NaCFCxfaI+QGDRrUUr8DACDQrwGZD5U2ZfXq1TJjxgwpKSmRX//613L48GH7s0HmWs6UKVPkpZdeuuF5wKuZa0Am0LgGBABtU4tcA7pZVpnAMR9WBQDgZpgLDgCgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAKgggAIAKAggAoIIAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgIlj8jGVZ9m2t1Ihc+REA0IbYr99XvZ63mQA6f/68fbtbPtVuCgDgFl/PIyMjr/u8x7pZRLWy+vp6OXXqlHTu3Fk8Ho/Xc5WVlZKYmCglJSUSEREhbsV+uIL9cAX74Qr2g//sBxMrJnwSEhIkKCio7fSATGO7d+9+w3XMTnXzAdaA/XAF++EK9sMV7Af/2A836vk0YBACAEAFAQQAUNGmAigsLEyWLFli37oZ++EK9sMV7Icr2A9tbz/43SAEAIA7tKkeEAAgcBBAAAAVBBAAQAUBBABQQQABAFS0mQBasWKF3H777dK+fXsZPny4fPHFF9pNanWvvPKKPT3R1Uv//v0l0O3atUsmTpxoT+thfudNmzZ5PW8Gci5evFji4+MlPDxcxo0bJ8eOHRO37YcZM2b87Ph46KGHJJBkZWXJ0KFD7am6YmJiZPLkyXL06FGvdS5duiRz586Vrl27SqdOnWTq1KlSVlYmbtsPKSkpPzseZs+eLf6kTQTQxx9/LBkZGfbY9gMHDsjgwYMlPT1dzpw5I24zYMAAOX36dOOye/duCXRVVVX2/7l5E9KUZcuWyfLly2XVqlWyd+9e6dixo318mBciN+0HwwTO1cfHunXrJJDk5+fb4VJYWCjbtm2TmpoaSUtLs/dNg4ULF8qWLVtk/fr19vpmbslHHnlE3LYfjJkzZ3odD+Zvxa9YbcCwYcOsuXPnNt6vq6uzEhISrKysLMtNlixZYg0ePNhyM3PIbty4sfF+fX29FRcXZ73xxhuNj5WXl1thYWHWunXrLLfsB2P69OnWpEmTLDc5c+aMvS/y8/Mb/+9DQkKs9evXN67zz3/+016noKDAcst+MB544AHr2WeftfyZ3/eALl++LPv377dPq1w9Yam5X1BQIG5jTi2ZUzC9evWSJ598Uk6cOCFuVlxcLKWlpV7Hh5kE0ZymdePxkZeXZ5+S6devn8yZM0fOnTsngayiosK+jYqKsm/Na4XpDVx9PJjT1D169Ajo46Himv3QYO3atRIdHS0DBw6UzMxMuXjxovgTv5sN+1pnz56Vuro6iY2N9Xrc3P/qq6/ETcyLanZ2tv3iYrrTS5culdGjR8vhw4ftc8FuZMLHaOr4aHjOLczpN3OqKSkpSY4fPy6//e1vZfz48fYLb7t27STQmK9uWbBggYwcOdJ+gTXM/3loaKh06dLFNcdDfRP7wXjiiSekZ8+e9hvWQ4cOyaJFi+zrRBs2bBB/4fcBhJ+YF5MGgwYNsgPJHGCffPKJPP3006ptg75p06Y1/nz33Xfbx0jv3r3tXlFqaqoEGnMNxLz5csN1UF/2w6xZs7yOBzNIxxwH5s2JOS78gd+fgjPdR/Pu7dpRLOZ+XFycuJl5l9e3b18pKioSt2o4Bjg+fs6cpjV/P4F4fMybN0+2bt0qO3fu9Pr+MPN/bk7bl5eXu+J4mHed/dAU84bV8Kfjwe8DyHSnhwwZIrm5uV5dTnM/OTlZ3OzChQv2uxnzzsatzOkm88Jy9fFhvhHSjIZz+/Fx8uRJ+xpQIB0fZvyFedHduHGj7Nixw/7/v5p5rQgJCfE6HsxpJ3OtNJCOB+sm+6EpBw8etG/96niw2oCPPvrIHtWUnZ1tHTlyxJo1a5bVpUsXq7S01HKT5557zsrLy7OKi4utzz//3Bo3bpwVHR1tj4AJZOfPn7e+/PJLezGH7Jtvvmn//N1339nP//73v7ePh82bN1uHDh2yR4IlJSVZP/74o+WW/WCee/755+2RXub42L59u3Xvvfdad9xxh3Xp0iUrUMyZM8eKjIy0/w5Onz7duFy8eLFxndmzZ1s9evSwduzYYe3bt89KTk62l0Ay5yb7oaioyPrd735n//7meDB/G7169bLGjBlj+ZM2EUDGO++8Yx9UoaGh9rDswsJCy20ee+wxKz4+3t4Hv/jFL+z75kALdDt37rRfcK9dzLDjhqHYL7/8shUbG2u/UUlNTbWOHj1quWk/mBeetLQ0q1u3bvYw5J49e1ozZ84MuDdpTf3+Zlm9enXjOuaNx29+8xvrtttuszp06GBNmTLFfnF20344ceKEHTZRUVH230SfPn2sF154waqoqLD8Cd8HBABQ4ffXgAAAgYkAAgCoIIAAACoIIACACgIIAKCCAAIAqCCAAAAqCCAAgAoCCACgggACAKgggAAAouF/AfkJR1kY+TCpAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_mnist(idx: int):\n",
    "    plt.figure()\n",
    "    plt.imshow(x_train[idx])\n",
    "    plt.title(y_train[idx])\n",
    "    plt.show()\n",
    "\n",
    "plot_mnist(1000)\n",
    "plot_mnist(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c223e3-a733-431a-b902-42c6311c5e7f",
   "metadata": {},
   "source": [
    "#### Perceptrón multicapa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5cebb2-cfa7-4ca7-9aab-58e834cb6845",
   "metadata": {},
   "source": [
    "Ahora que ya hemos ojeado un poco el dataset, vamos a ver como podemos crear nuestra red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8642cc6-5147-4079-9807-9800e6d66a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5b30d4-542d-4da1-b92d-5e79b316e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, in_features: int):\n",
    "        self.in_features = in_features\n",
    "\n",
    "        self.weights = np.random.uniform(-1, 1, (in_features,))\n",
    "        self.bias = 0.0\n",
    "\n",
    "    \n",
    "    def forward(self, X: np.ndarray):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2775eba8-0fa6-416a-85e8-7fa7615560b5",
   "metadata": {},
   "source": [
    "Aquí merece la pena detenerse un momento. Por lo que hemos visto, los pesos son el motor de la red, puesto que esta aprende ajustandolos. Si nos fijamos, hemos definidos estos como un array con valores aleatorios entre -1 y 1. Es interesante entender que esta inicialización puede hacer que la red aprenda mejor o peor, pues si el set de pesos aleatorios $W$ queda muy lejos de nuestro set \"óptimo\" $W_{opt}$, la red no aprenderá bien o tardará mucho más."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619dee0c-f51c-4bcd-bed6-62f3bcec059a",
   "metadata": {},
   "source": [
    "Existen ciertas inicializaciones de pesos, que son matemáticamente más correctas. Vamos a usar una conocida como inicialización He, la cual está diseñada para usar con la función de activación ReLU y consiste en inicializar pesos aleatorios pero ajustar su varianza como $$ \\sigma^2 = 2/N$$\n",
    "Donde $N$ es el número de entradas a la neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e8d228c-55db-491b-bedc-4a47724b70c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_init(fan_in: int):\n",
    "    return np.random.randn(fan_in) * np.sqrt(2.0 / fan_in)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3ee935-110f-45d8-97b8-6057c5fb0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, in_features: int):\n",
    "        self.in_features = in_features\n",
    "\n",
    "        self.weights = he_init(fan_in=in_features)\n",
    "        self.bias = 0.0\n",
    "\n",
    "    \n",
    "    def forward(self, X: np.ndarray):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b263860e-8d25-4152-8dcc-53a4285e3285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.22213732 -0.06183368  0.28965512  0.68111966 -0.10471657 -0.10470923\n",
      "  0.70624544  0.34320724 -0.20995533  0.24264023]\n"
     ]
    }
   ],
   "source": [
    "neuron = Neuron(in_features=10)\n",
    "print(neuron.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a180dc-7c8a-4908-b1e1-cadb8ad7af71",
   "metadata": {},
   "source": [
    "Procedemos, una vez que sabemos como inicializar una neurona, a implementar la acción que realiza (la ecuación que hemos visto antes). Para esto, procedemos a definir la función ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e49ffd-ba25-47e8-aa0a-9bc7ea164e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x: float):\n",
    "    return max(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb025532-abdc-4acd-b83a-da254c61d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, in_features: int):\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.weights = he_init(fan_in=in_features)\n",
    "        self.bias = 0.0\n",
    "\n",
    "        \n",
    "    def forward(self, X: np.ndarray):\n",
    "        # Dada una entrada X, nuestra neurona aplica la suma ponderada y después la función ReLU\n",
    "        z = self.weights @ X + self.bias  # @ significa multiplicación de matrices\n",
    "        z = relu(z)\n",
    "        return z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9a4bafb-e8dd-4db1-8abf-d02f82561647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3639929142211944\n"
     ]
    }
   ],
   "source": [
    "neuron = Neuron(in_features=10)\n",
    "print(neuron.forward(X=np.random.randn(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95838549-6105-41b6-b875-850d90d0fa77",
   "metadata": {},
   "source": [
    "Ahora que podemos definir neuronas, toca aprender a definir capas enteras (conjunto de neuronas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a41a110-7d0f-4318-b842-30ee61ed5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_inputs: int, n_neurons: int):\n",
    "        \"\"\" n_inputs: número de entradas para cada una de las neuronas (que es igual al número de neuronas en la capa anterior)\n",
    "            n_neurons: número de neuronas en la capa \"\"\"\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "\n",
    "        self.neurons = [Neuron(n_inputs) for i in range(n_neurons)]\n",
    "\n",
    "    def forward(self, X: np.ndarray):\n",
    "        return np.array([neuron.forward(X) for neuron in self.neurons])  # Array 1D con shape (n_neurons,)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer(n_inputs={self.n_inputs}, n_neurons={self.n_neurons})\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a15659-9c0c-49dc-85ec-3c8f31cf3f6a",
   "metadata": {},
   "source": [
    "Una vez sabemos como definir capas, ahora toca construir nuestro Perceptrón Multicapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2944831e-b6f6-4ae1-aa33-653d65529098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, sizes: list[int]):\n",
    "        \"\"\"\n",
    "        sizes: número de neuronas de cada capa\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.n_layers = len(sizes) - 1  # -1 ya que no contamos con la de entrada\n",
    "        self.layers = []\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            in_features = sizes[i]\n",
    "            out_features = sizes[i + 1]\n",
    "            self.layers.append(Layer(n_inputs=in_features, n_neurons=out_features))\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da0a4f7e-66ea-464d-b0ee-f4c6b2a05cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP([10,20,30,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67373987-6e9e-4667-b86b-efae3c1acf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Layer(n_inputs=10, n_neurons=20), Layer(n_inputs=20, n_neurons=30), Layer(n_inputs=30, n_neurons=10)]\n"
     ]
    }
   ],
   "source": [
    "print(mlp.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cbf53ca-c5e4-4cd4-80e3-81e7d57f9a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.93322177, 3.1137477 , 0.        , 0.        ,\n",
       "       1.50826623, 0.        , 0.        , 0.37490697, 1.4783555 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.forward(X=np.random.randn(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1fcb3a-e5ea-4855-9622-663796ec905d",
   "metadata": {},
   "source": [
    "Con esto, ya hemos concluido casi todo el primer paso de nuestro algoritmo. Hemos sido capaces de definir todo el flujo que sigue un dato desde que entra a la red hasta que sale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c628d-7e95-4cb1-bad0-a043ddcc3c27",
   "metadata": {},
   "source": [
    "Lo siguiente, es definir una función de pérdidas que sea capaz de indicarnos cuanto se ha equivocado nuestro modelo. Las funciones de pérdidas son dependientes de la tarea que queremos resolver, y generalmente, para clasificación, se suele usar la función CrossEntropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd674efc-418b-4333-9d28-345601455881",
   "metadata": {},
   "source": [
    "Sin embargo, puesto que nuestro objetivo es didáctico y no de rendimiento, usaremos la función MSE, la cual toma la forma \n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "1. n es el número de datos\n",
    "2. $Y_i$ son los valores reales\n",
    "3. $\\hat{Y}_i$ son los valores predichos por nuestra red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c768b32-0f8c-46b1-844d-ecdfadc60d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_pred: np.array, y_true: np.array):\n",
    "    return np.square(np.subtract(y_true, y_pred)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c316e90-756a-42b9-b7d9-1783a56236a5",
   "metadata": {},
   "source": [
    "Ahora vamos a mandar un dato real del dataset mnist, vamos a ejecutar el forward con este dato, y vamos a aplicar la función de pérdidas. Si esto funciona, simplemente queda implementar el segundo paso y podremos facilmente entrenar nuestar red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e20e3378-be37-4bac-ab00-264c58077ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27839162 0.39019326 0.         0.         0.33204118 0.72728531\n",
      " 1.51360928 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "image = x_train[1000]\n",
    "label = y_train[1000]\n",
    "\n",
    "\n",
    "# Definimos nuestro MLP\n",
    "mlp = MLP([28*28, 120, 10])  # 3 capas, la primera con tantas neuronas como pixeles tiene una imagen de mnist, la ultima con el número de clases\n",
    "\n",
    "logits = mlp.forward(image.ravel() / 255.0)  # Normalizamos entr 0-1\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78abef91-a93e-4740-80b0-e4b6ee733673",
   "metadata": {},
   "source": [
    "Estos valores a la salida del paso forward, si nos fijamos, no son probabilidades, puesto que no suman 1. Nuestra idea es que nuestra red nos de una distribución de probabilidades, donde la probabilidad de la clase correcta sea la más alta. Para convertir estos números a una distribución de probabilidad se usa la función softmax:\n",
    "\n",
    "$$ softmax(x_i) = \\frac{e^{x_i}}{\\sum_{j}e^{x_j}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c98cc587-bd81-46e8-badb-8ab5639aa71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits: np.array) -> np.array:\n",
    "    exp = np.exp(logits - np.max(logits))  # la resta del máximo es para estabilidad numérica\n",
    "    return exp / np.sum(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0a5b499-c0d1-4d17-8f8a-85f560807ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0835833  0.09347045 0.06327258 0.06327258 0.08818998 0.13093977\n",
      " 0.28745358 0.06327258 0.06327258 0.06327258]\n"
     ]
    }
   ],
   "source": [
    "probs = softmax(logits)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b2848-c1ad-4a0f-9665-d8af448b528a",
   "metadata": {},
   "source": [
    "Finalmente, podemos aplicar la función de pérdidas que hemos definido antes a esta distribución de probabilidad. Para ello, nuestra etiqueta debe ser un vector \"one-hot\", lo cual quiere decir que debe ser codificado de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ad1e4f5-63b2-4c4e-b2ab-4a1b8273fccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para la etiqueta 0 el vector codificado es [1 0 0 0 0 0 0 0 0 0]\n",
      "Para la etiqueta 1 el vector codificado es [0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def make_onehot_label(label: int):\n",
    "    return np.array([1 if i == label else 0 for i in range(10)])\n",
    "\n",
    "print(f\"Para la etiqueta 0 el vector codificado es {make_onehot_label(0)}\")\n",
    "print(f\"Para la etiqueta 1 el vector codificado es {make_onehot_label(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8017600f-3b9a-4955-b574-b36b125187f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09761256510728968\n"
     ]
    }
   ],
   "source": [
    "loss = mse_loss(y_pred=probs, y_true=make_onehot_label(label))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640e8dd-dc50-4198-998a-9e68fabc6b79",
   "metadata": {},
   "source": [
    "¡Y listo! ¡Ya tenemos definida toda la primera parte del algoritmo comentado antes! Somos capaces de definir una red de tamaño arbitrario, enviar una imagen a través de ella, convertir sus salidas a probabilidades, y medir el error de la red. Ahora queda la segunda parte, definir la propagacion de este error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb8ec0-14d6-4ed4-9018-9ed0f83bdf8a",
   "metadata": {},
   "source": [
    "Para esto, vamos a reescribir las clases, modificando pequeñas partes y añadiendo la función \"backward\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b3c0985-41cc-41b1-a0f0-0a296419ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, in_features: int):\n",
    "        self.in_features = in_features\n",
    "        \n",
    "        self.weights = he_init(fan_in=in_features)\n",
    "        self.bias = 0.0\n",
    "\n",
    "        self.cache = {}  # Aquí almacenamos nuestra entrada, nuestra combinación lineal con pesos, y nuestra salida\n",
    "\n",
    "    def forward(self, X: np.ndarray):\n",
    "        # Dada una entrada X, nuestra neurona aplica la suma ponderada y después la función ReLU\n",
    "        z = self.weights @ X + self.bias  # @ significa multiplicación de matrices\n",
    "        a = relu(z)\n",
    "\n",
    "        # guardado de nuestro valores\n",
    "        self.cache[\"A\"] = a\n",
    "        self.cache[\"Z\"] = z\n",
    "        self.cache[\"X\"] = X\n",
    "        \n",
    "        return a\n",
    "\n",
    "    def backward(self, grad_out: float, lr: float):\n",
    "        \"\"\" \n",
    "        grad_out: el gradiente que proviene de la capa siguiente\n",
    "        lr: el ratio de aprendizaje\n",
    "        \"\"\"\n",
    "        X, Z, A = self.cache[\"X\"], self.cache[\"Z\"], self.cache[\"A\"]\n",
    "\n",
    "        # Lo primero que hacemos es quitar la función de activación (pasar de A a Z)\n",
    "        dZ = grad_out * (1.0 if Z > 0 else 0.0)  # grad_out * (la derivada de la relu)\n",
    "\n",
    "        # Ahora calculamos la derivada de la pérdida con respecto de los pesos y del bias\n",
    "        dW = dZ * X\n",
    "        db = dZ\n",
    "\n",
    "        # Gradiente que enviamos hacia la capa anterior\n",
    "        dX = dZ * self.weights\n",
    "\n",
    "        # Actualizamos pesos minimizando gradiente\n",
    "        self.weights -= lr * dW\n",
    "        self.bias -= lr * db\n",
    "\n",
    "        return dX\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b101b43d-5a54-4d30-8841-e0b8ad7c60c6",
   "metadata": {},
   "source": [
    "Para arrojar un poco de luz, esto no es mas que hacer uso de la regla de la cadena:\n",
    "\n",
    "1. Queremos conocer la derivada de la pérdida con respecto a los pesos, para saber como afecta cada peso al error y modificarlo para minimizar este error.\n",
    "2. La fórmula que describe esto es$$\n",
    "\\frac{\\partial L}{\\partial W_i}\n",
    "= \n",
    "\\frac{\\partial L}{\\partial A} \\cdot\n",
    "\\frac{\\partial A}{\\partial Z} \\cdot\n",
    "\\frac{\\partial Z}{\\partial W_i}\n",
    "$$ Donde: $\\frac{\\partial L}{\\partial A}$ es grad_out (lo que cambia la pérdida si cambia la salida de la neurona), $\\frac{\\partial A}{\\partial Z}$ es la derivada de la función ReLU, y $\\frac{\\partial Z}{\\partial W_i}$, dado que $Z = W_i \\cdot X_i + b$, no es más que $X_i$\n",
    " \n",
    "3. Por esto, $\\frac{\\partial L}{\\partial W_i}$  = $grad_{out}$ $\\cdot$ (1.0 if Z > 0 else 0.0) $\\cdot$ $X$\n",
    "4. Para $\\frac{\\partial L}{\\partial b}$, solo cambia la última derivada que en lugar de ser $\\frac{\\partial Z}{\\partial W_i}$ es $\\frac{\\partial Z}{\\partial b}$, que es 1 si atendemos a $Z = W_i \\cdot X_i + b$\n",
    "5. Cada neurona hace dos cosas, la primera es actualizar sus pesos, restandose a si mismo $\\frac{\\partial L}{\\partial W_i}$ multiplicada por una \"tasa de aprendizaje\", que indica como de brusca será nuestar actualización de pesos. La segunda es enviar un gradiente hacia la capa anterior, el cual le dice cuanto afectó su salida al error total.\n",
    "6. Este gradiente que se envía hacia atrás no es más que $\\frac{\\partial L}{\\partial X}$, que es $grad_{out}$ $\\cdot$ (1.0 if Z > 0 else 0.0) $\\cdot$ $W$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4ed29f-02de-4d53-816b-7b71bf82005a",
   "metadata": {},
   "source": [
    "Una vez explicado como hacer backward en cada neurona, procedemos a implementar el método en la capa. Este simplemente calculará el backward de cada neurona y devolverá la suma de todas hacia la capa anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8514e42c-f4cd-4ffa-8c41-384b1cdae126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_inputs: int, n_neurons: int):\n",
    "        \"\"\" n_inputs: número de entradas para cada una de las neuronas (que es igual al número de neuronas en la capa anterior)\n",
    "            n_neurons: número de neuronas en la capa \"\"\"\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "\n",
    "        self.neurons = [Neuron(n_inputs) for i in range(n_neurons)]\n",
    "\n",
    "    def forward(self, X: np.ndarray):\n",
    "        return np.array([neuron.forward(X) for neuron in self.neurons])  # Array 1D con shape (n_neurons,)\n",
    "\n",
    "    def backward(self, grad: np.ndarray, lr: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        grad: vector (n_neurons,) con dL/dA_k para cada neurona k de la capa.\n",
    "        Devuelve dX_total: vector (n_inputs,) para enviar a la capa anterior.\n",
    "        \"\"\"\n",
    "        dX_total = np.zeros(self.n_inputs, dtype=float)\n",
    "        for gk, neuron in zip(grad, self.neurons):\n",
    "            # Cada neurona devuelve su dX_k; sumamos todas las contribuciones\n",
    "            dX_total += neuron.backward(float(gk), lr)\n",
    "        return dX_total \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Layer(n_inputs={self.n_inputs}, n_neurons={self.n_neurons})\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb2ce61-037b-45d3-819b-c3e21cc0469a",
   "metadata": {},
   "source": [
    "Y finalmente en la clase MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5cebcd7e-907e-4de6-9bc0-ba1a74f2ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, sizes: list[int]):\n",
    "        \"\"\"\n",
    "        sizes: número de neuronas de cada capa\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.n_layers = len(sizes) - 1  # -1 ya que no contamos con la de entrada\n",
    "        self.layers = []\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            in_features = sizes[i]\n",
    "            out_features = sizes[i + 1]\n",
    "            self.layers.append(Layer(n_inputs=in_features, n_neurons=out_features))\n",
    "\n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, grad_out: np.ndarray, lr: float) -> np.ndarray:\n",
    "        G = grad_out\n",
    "        for layer in reversed(self.layers):\n",
    "            G = layer.backward(G, lr)   # cada capa devuelve dX para la anterior\n",
    "        return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f057c666-00c8-4302-b795-1e99e6a28b8c",
   "metadata": {},
   "source": [
    "Esto sería todo, salvo que queda una pequeña cosa. Nuestro método backward calcula el gradiente para las capas de la red, pero no tiene en cuenta como hacerlo para la última capa, pues esto depende de las softmax y del error. Para esto definimos la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9df137b-687e-4811-8f0e-5be77bd37d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_mse(probs: np.array, labels: np.array):\n",
    "    grad_a = 2 * (probs - labels)  # mirar definición de mse y derivar frente a probs\n",
    "\n",
    "    # quitar softmax\n",
    "    dot = np.dot(grad_a, probs)                # escalar\n",
    "    grad_z = probs * (grad_a - dot)\n",
    "\n",
    "    return grad_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3a39f0-eee4-43d9-9f95-9f5632dbd376",
   "metadata": {},
   "source": [
    "¡Ahora sí! Ya tenemos todas las piezas y podemos crear un bucle de entrenamiento para ver si nuestra red aprende a hacer esta tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637edcaf-96a6-475a-b3d2-0ff60b4bd520",
   "metadata": {},
   "source": [
    "Generalmente, un bucle de entrenamiento es mucho más sofisticado, y por ejemplo, se envían los datos a la red en lotes, lo cual hace que el entrenamiento sea más estable y las redes aprendan de más ejemplos por iteración. Sin embargo, nosotros vamos a hacer las cosas sencillas y vamos a enviar las imágenes una a una."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39afd80d-19aa-4b2d-9e3a-c639cc9ca908",
   "metadata": {},
   "source": [
    "Vamos a definir una clase dataset, la cual se encargará de gestionar los datos del dataset MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9d1750a-1fbd-46e4-91d3-0058de7fed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, x: np.array, y: np.array):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        image = self.x[idx].ravel() / 255.0\n",
    "        label = make_onehot_label(self.y[idx])\n",
    "        return image, label    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644d2f6-8a38-44ea-be8d-6ff43cb59221",
   "metadata": {},
   "source": [
    "Ahora procedemos a crear nuestro bucle de entrenamiento, que simplemente es usar los pasos que ya hemos definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "076399cc-c4d5-402e-9fe8-e072eaa5b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, lr, dataset):\n",
    "    losses = []\n",
    "    for i in range(len(dataset)):\n",
    "        image, label = dataset[i]  # extraemos datos del dataset\n",
    "        logits = model.forward(image)  # enviamos al modelo\n",
    "        probs = softmax(logits)  # calculamos distribución de probabilidad\n",
    "        loss = mse_loss(y_pred=probs, y_true=label)  # sacamos el error\n",
    "        losses.append(loss)\n",
    "        \n",
    "        grad = d_mse(probs=probs, labels=label)\n",
    "        model.backward(grad_out=grad, lr=lr)\n",
    "\n",
    "    print(f\"Mean loss in epoch is {np.mean(np.array(losses))}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def test(model, dataset):\n",
    "    losses = []\n",
    "    for i in range(len(dataset)):\n",
    "        image, label = dataset[i]\n",
    "        logits = model.forward(image)\n",
    "        probs = softmax(logits)\n",
    "        loss = mse_loss(y_pred=probs, y_true=label)\n",
    "        losses.append(loss)\n",
    "    print(f\"Mean loss in epoch is {np.mean(np.array(losses))}\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0703d41-f819-44ba-824c-75bbb5a0a55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 10\n",
      "Mean loss in epoch is 0.03502896172086226\n",
      "Mean loss in epoch is 0.02294830420443723\n",
      "Epoch 2 out of 10\n",
      "Mean loss in epoch is 0.021550378037060555\n",
      "Mean loss in epoch is 0.02001081745825722\n",
      "Epoch 3 out of 10\n",
      "Mean loss in epoch is 0.019339678690497894\n",
      "Mean loss in epoch is 0.018620345290762366\n",
      "Epoch 4 out of 10\n",
      "Mean loss in epoch is 0.018018027066018266\n",
      "Mean loss in epoch is 0.017689882019464424\n",
      "Epoch 5 out of 10\n",
      "Mean loss in epoch is 0.017050428096633877\n",
      "Mean loss in epoch is 0.01700240601490804\n",
      "Epoch 6 out of 10\n",
      "Mean loss in epoch is 0.016290885349972195\n",
      "Mean loss in epoch is 0.016444158217062663\n",
      "Epoch 7 out of 10\n",
      "Mean loss in epoch is 0.01567834271037792\n",
      "Mean loss in epoch is 0.01599143129363492\n",
      "Epoch 8 out of 10\n",
      "Mean loss in epoch is 0.015174842633987468\n",
      "Mean loss in epoch is 0.015614485267687073\n",
      "Epoch 9 out of 10\n",
      "Mean loss in epoch is 0.014750804137048249\n",
      "Mean loss in epoch is 0.01530035171341647\n",
      "Epoch 10 out of 10\n",
      "Mean loss in epoch is 0.014389692738088588\n",
      "Mean loss in epoch is 0.015031494337676981\n"
     ]
    }
   ],
   "source": [
    "model = MLP([28*28, 120, 10])\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i+1} out of {epochs}\")\n",
    "    model = train(model=model, lr=lr, dataset = dataset_train)\n",
    "    test(model=model, dataset=dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c560539d-4183-42a3-929f-7e9901de8785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuestro modelo tiene una accuracy de 90.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def random_pred(model, dataset_test):\n",
    "    \"\"\" Esta funcion usa el modelo entrenado para predecir sobre una imagen aleatoria\n",
    "    y muestra el resultado \"\"\"\n",
    "\n",
    "    random_idx = random.randint(1, len(dataset_test)-1)\n",
    "    image, label = dataset_test[random_idx]\n",
    "\n",
    "    logits = model.forward(image)\n",
    "    probs = softmax(logits)\n",
    "    predicted = np.argmax(probs)\n",
    "    true = np.argmax(label)\n",
    "\n",
    "    if true == predicted:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# 30 predicciones\n",
    "preds = []\n",
    "for i in range(30):\n",
    "    preds.append(random_pred(model=model, dataset_test=dataset_test))\n",
    "    \n",
    "accuracy = (np.sum(preds) / len(preds)) * 100\n",
    "print(f\"Nuestro modelo tiene una accuracy de {accuracy}\")      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
